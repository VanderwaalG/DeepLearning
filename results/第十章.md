```
#linux可以运行
import torch
import torch.nn as nn
from torch import optim
from pytorch_transformers import BertTokenizer, BertModel
from transformers import BertTokenizerFast
from torch.utils.data import Dataset, DataLoader
import json
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MAX_LEN = 512

# 使用绝对路径
base_dir = os.path.dirname(os.path.abspath(__file__))
bert_model_dir = os.path.join(base_dir, 'Bert_model')
tokenizer_path = os.path.join(base_dir, 'tokenizer')

# 确保Bert_model目录存在
os.makedirs(bert_model_dir, exist_ok=True)

# 加载tokenizer
try:
    tokenizer = torch.load(tokenizer_path)
    print("Loaded tokenizer from file")
except:
    print("Downloading and saving tokenizer")
    tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')
    torch.save(tokenizer, tokenizer_path)

# 从json文件中读取问题文本、文本段、答案的起始位置，一条样本放在一个四元组中，返回多个四元组的list
def get_query_passage_answer(fn):
    with open(fn, 'r', encoding='utf-8') as reader:
        data = json.load(reader)['data']
    examples = []
    data = data[0] #len(data)=1
    for paragraph in data['paragraphs']: #data['paragraphs']是长度为100的list
        paragraph_text = paragraph['context']  #篇章的内容
        qa = paragraph['qas'][0] #paragraph['qas']是长度为1的list，里面有一个字典
        query = qa['question'] #问题文本
        id = qa['id']
        if len(query+paragraph_text)+3>MAX_LEN:
            continue
        answer = qa['answers'][0]['text']  #答案文本
        answer_start = qa['answers'][0]['answer_start'] #起始位置索引
        answer_end = answer_start+len(answer)           #终止位置索引
        item = (query,paragraph_text,answer_start,answer_end)
        examples.append(item)
    return examples

class MyDataSet(Dataset):
    def __init__(self, query_passage_answer):
        super(MyDataSet, self).__init__()
        self.query_passage_answer = query_passage_answer
    def __len__(self):
        return len(self.query_passage_answer)
    def __getitem__(self, idx):
        query_passage_answer = self.query_passage_answer[idx]
        query = query_passage_answer[0]  #获得问题文本
        passage = query_passage_answer[1] #获得篇章文本
        answers_start = query_passage_answer[2] #获得答案的起始位置索引
        answers_end = query_passage_answer[3]  #获得答案的终止位置索引
        #answers = passage[answers_start:answers_end]
        # 对篇章文本进行索引编码，同时返回编码前后位置索引之间的关系信息
        tokenizing_result = tokenizer.encode_plus(passage,
                            return_offsets_mapping=True,
                            add_special_tokens=False)
        #对问题文本进行分词和索引编码
        query_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(query))
        passage_ids = tokenizing_result['input_ids'] #获取篇章文本的索引编码
        token_span = tokenizing_result['offset_mapping'] #获得编码前后位置索引之间的关系信息
        query_ids = [101]+query_ids+[102] #在问题文本前后分别加上[CLS]和[SEP]的索引
        passage_ids = passage_ids+[102] #在篇章文本编码后面加上[SEP]的索引
        sen1_len = len(query_ids) #输入BERT的句子长度
        sen2_len = len(passage_ids) #填充长度
        sen_len = sen1_len + sen2_len

        input_ids = query_ids + passage_ids + [0] * (MAX_LEN - sen_len) #填充[PAD]，0为[PAD]的索引
        #(1)构造问题+篇章的索引编码
        input_ids = torch.tensor(input_ids)

        token_type_ids = [0] * sen1_len + [1] * (MAX_LEN - sen1_len)#(3)构造句子掩码向量 torch.Size([512]
        attention_mask = [1] * sen_len + [0] * (MAX_LEN - sen_len)#(2)构造注意力掩码向量 torch.Size([512]
        token_type_ids = torch.tensor(token_type_ids)
        attention_mask = torch.tensor(attention_mask)
        # passage----->
        # token_span----->
        #建立分词前后之间字符位置的索引映射关系
        CharInd_TokenInd = [[] for _ in range(len(passage)+1)]
        CharInd_TokenInd[len(passage)] = [len(passage)]
        for token_ind, char_sp in enumerate(token_span):
            for text_ind in range(char_sp[0], char_sp[1]):
                CharInd_TokenInd[text_ind] += [token_ind]
        for k in range(len(CharInd_TokenInd) - 2, -1, -1):  # 填补空格
            if CharInd_TokenInd[k] == []:
                CharInd_TokenInd[k] = CharInd_TokenInd[k + 1]

        answers_start_ids = sen1_len + CharInd_TokenInd[answers_start][0]
        answers_end_ids = sen1_len + CharInd_TokenInd[answers_end][0]
        labels = [answers_start_ids,answers_end_ids]  #(4)构造答案的起始位置索引张量（标签）
        labels = torch.tensor(labels)
        return  input_ids, attention_mask, token_type_ids, labels
        #返回用于BERT的文本索引编码（问题+篇章文本的索引编码）、注意力掩码向量、句子掩码向量和答案的起始位置索引张量（标签）

class BertForReading(nn.Module):
    def __init__(self): #
        super(BertForReading, self).__init__()
        # 加载预训练模型
        self.model = BertModel.from_pretrained('bert-base-chinese',
                     cache_dir=bert_model_dir).to(device)#加载模型
        self.qa_outputs = nn.Linear(768, 2)  # 有两个预测任务
    def forward(self, b_input_ids, b_attention_mask, b_token_type_ids):  #
        outputs = self.model(input_ids=b_input_ids,attention_mask=b_attention_mask,token_type_ids=b_token_type_ids)
        sequence_output = outputs[0]
        logits = self.qa_outputs(sequence_output) #torch.Size([8, 512, 2])
        return logits

#-----------------------------------------------
# 使用绝对路径
path = os.path.join(base_dir, 'data', 'dureader_robust-data')
name = 'train.json'
fn = os.path.join(path, name)

# 确保数据目录存在
os.makedirs(os.path.join(base_dir, 'data', 'dureader_robust-data'), exist_ok=True)

query_passage_answer = get_query_passage_answer(fn)  #样本数量为12649

mydataset = MyDataSet(query_passage_answer)
data_loader = DataLoader(mydataset, batch_size=8, shuffle=True) #打包数据集
print('数据集大小：',len(data_loader.dataset))
reading_model = BertForReading().to(device) #实例化
optimizer = optim.Adam(reading_model.parameters(), lr=1e-5)

#开始训练
for ep in range(10):
    for k, (b_input_ids, b_attention_mask, b_token_type_ids, b_labels) in enumerate(data_loader):
        b_input_ids, b_attention_mask = b_input_ids.to(device), b_attention_mask.to(device)
        b_token_type_ids, b_labels = b_token_type_ids.to(device), b_labels.to(device)

        logits = reading_model(b_input_ids, b_attention_mask, b_token_type_ids)
        #logits的形状：torch.Size([8, 512, 2])
        start_logits = logits[:, :, 0] #torch.Size([8, 512])
        end_logits = logits[:, :, 1]
        #ignore_index = start_logits.size(1) #序列长度  512
        #loss_fct = nn.CrossEntropyLoss(ignore_index=ignore_index)
        loss_fun = nn.CrossEntropyLoss()

        start_label = b_labels[:, 0]  #torch.Size([8])
        end_label = b_labels[:, 1]

        start_loss = loss_fun(start_logits, start_label)
        end_loss = loss_fun(end_logits, end_label)
        loss = (start_loss + end_loss) / 2
        if k%10==0:
            print(ep,k,len(data_loader),loss.item())
        optimizer.zero_grad()
        loss.backward()  # 会积累梯度
        optimizer.step()

# 保存模型
model_save_path = os.path.join(base_dir, 'reading_model2')
torch.save(reading_model, model_save_path)

# 评估模型
reading_model = torch.load(model_save_path)
reading_model.eval()
correct = 0
for k, (b_input_ids, b_attention_mask, b_token_type_ids, b_labels) in enumerate(data_loader):
    b_input_ids, b_attention_mask = b_input_ids.to(device), b_attention_mask.to(device)
    b_token_type_ids, b_labels = b_token_type_ids.to(device), b_labels.to(device)
    # torch.Size([8, 512]) torch.Size([8, 512]) torch.Size([8, 512]) torch.Size([8, 2])
    logits = reading_model(b_input_ids, b_attention_mask, b_token_type_ids)
    #torch.Size([8, 512, 2])
    start_logits, end_logits = logits.split(1, dim=-1) #torch.Size([8, 512, 1]) torch.Size([8, 512, 1])
    start_logits = start_logits.squeeze(-1)  #   torch.Size([8, 512])
    end_logits = end_logits.squeeze(-1)  # torch.Size([8, 512])

    pre_start_pos = torch.argmax(start_logits, dim=1).long()  # torch.Size([8])
    pre_end_pos = torch.argmax(end_logits, dim=1).long()

    start_positions = b_labels[:, 0]  # torch.Size([8])
    end_positions = b_labels[:, 1]

    t1 = (pre_start_pos == start_positions).byte()
    t2 = (pre_end_pos == end_positions).byte()
    t = (t1*t2).sum()
    correct += t
    if k%5 == 0:
        print('预测完成率：',round(1.*k/len(data_loader),4))

    #break
correct = 1.*correct/len(data_loader.dataset)
print('准确率：',round(correct.item(),3))
```











3060 12G

![image-20250822083016336](C:\Users\Gasoline\AppData\Roaming\Typora\typora-user-images\image-20250822083016336.png)

ex1

```python
import torch
from torch.utils.data import Dataset
import torch.nn as nn
from torch.utils.data import DataLoader
from PIL import Image
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from transformers import  AutoTokenizer ,BertTokenizer
import torch.optim as optim
#from pytorch_transformers import BertModel
from transformers import AlbertModel ,AlbertTokenizer
from torchvision import transforms #, models  datasets,
from efficientnet_pytorch import EfficientNet
import os
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def readTxtFile(fn): #读指定文件的内容
    fg = open(fn, 'r', encoding='gb18030')  #读中文文本
    text = list(fg)
    assert len(text)==1
    text = text[0]
    text = text.replace('\n','')
    return text

def get_txt_img_lb(path, txtname):
    fn = os.path.join(path, txtname)  # 使用os.path.join
    with open(fn, encoding='utf-8') as fg:  # 使用with语句更安全
        samples = []
        for line in fg:
            line = line.strip()
            if 'ID' in line:
                continue
            file_id, label = line.split(',')
            # 使用os.path.join构建路径
            text_path = os.path.join(path, 'data', file_id + '.txt')
            img_path = os.path.join(path, 'data', file_id + '.jpg')
            text = readTxtFile(text_path)
            item = (text, img_path, label)
            samples.append(item)
    return samples


tokenizer = AutoTokenizer.from_pretrained("./bert") #执行该语句要比较久的时间albert-base-v2
tsf =  transforms.Compose([transforms.RandomResizedCrop(224),
                    transforms.RandomHorizontalFlip(),
                    transforms.ToTensor(),
                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

class MyDataSet(Dataset):
    def __init__(self, samples):
        self.samples = samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        text, img_path, label = self.samples[idx]
        text_list = [text]
        #索引编码：
        txtdata = tokenizer.batch_encode_plus(batch_text_or_text_pairs=text_list,
                                       truncation=True,
                                       padding='max_length',
                                       max_length=128,  # 2022.7.29 由于sg数据训练报错 超出索引范围 怀疑max过长导致超出
                                       return_tensors='pt',
                                       return_length=True)
        input_ids = txtdata['input_ids']
        token_type_ids = txtdata['token_type_ids']
        attention_mask = txtdata['attention_mask']

        img = Image.open(img_path)
        if img.mode != 'RGB':
            print('不是RGB图像！')
            exit(0)
        img = tsf(img)  #改变形状为torch.Size([3, 224, 224])
        label = int(label)

        #                    torch.Size([128])        torch.Size([3, 224, 224])         int
        return input_ids[0],token_type_ids[0],attention_mask[0],        img,          label


bert_model = AlbertModel.from_pretrained('./bert').to(device)
# 【此处为本例部分核心代码，已省略，完整代码见教材P293；建议读者手工输入核心代码并进行调试，这样方能领会其含义】
# 创建 EfficientNet 模型并加载本地权重
effi_model = EfficientNet.from_name('efficientnet-b7').to(device)
try:
    # 尝试从本地加载预训练权重
    state_dict = torch.load("./EfficientNet_model/efficientnet-b7.pth")
    effi_model.load_state_dict(state_dict)
    print("成功加载本地 EfficientNet 预训练权重")
except:
    print("无法加载本地 EfficientNet 预训练权重，使用随机初始化")

# 冻结所有参数
for e in effi_model.parameters():
    e.requires_grad = False

# 修改最后一层
effi_model._fc = nn.Linear(2560, 768)

class Multi_Model(nn.Module):  #定义深度神经网络模型类
    def __init__(self):
        super().__init__()
        self.bert_model = bert_model
        self.effi_model = effi_model
        self.fc = nn.Linear(768 + 768, 3)
    def forward(self,data):
        input_ids, token_type_ids, attention_mask, img, _ = data
        input_ids, token_type_ids, attention_mask, img = input_ids.to(device), \
                                 token_type_ids.to(device), attention_mask.to(device), img.to(device)


        #torch.Size([8, 128]) torch.Size([8, 128]) torch.Size([8, 128]) torch.Size([8, 3, 224, 224])

        outputs = self.bert_model(input_ids=input_ids,
                                  attention_mask=attention_mask,
                                  token_type_ids=token_type_ids)
        text_feature = outputs[1]  #文本的特征，形状为torch.Size([8, 768])
        effi_outputs = self.effi_model(img)  #图像的特征，形状为torch.Size([8, 768])
        cat_feature = torch.cat([text_feature, effi_outputs], -1)  # 采用拼接融合方式，cat_feature的形状为torch.Size([8, 1536])
        out = self.fc(cat_feature)  # torch.Size([16, 3])
        return out

def train(model:Multi_Model, data_loader):  #对模型进行训练
    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6, \
amsgrad=False)
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10,\
                          T_mult=1, eta_min=1e-6, last_epoch=-1)
    criterion = nn.CrossEntropyLoss()
    lr = scheduler.get_last_lr()[0]
    print('epochs :0   lr:{}'.format(lr))
    print('训练中..........')
    epochs = 31
    for ep in range(epochs):
        for k,data in enumerate(data_loader):
            input_ids, token_type_ids, attention_mask, img, label = data
            label = label.to(device)
            pre_y = model(data)
            loss = criterion(pre_y, label)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        scheduler.step()
        lr = scheduler.get_last_lr()[0]
        if not ep + 1 == epochs:
            print('epochs :{}   lr:{:.6f}'.format(ep + 1, lr))
        if ep % 5 == 0:  #每5轮循环保存一次模型参数
            torch.save({'model_state_dict': model.state_dict()}, f'multi_model_new.pt')
            check_point = torch.load(f'multi_model_new.pt')
            model.load_state_dict(check_point['model_state_dict'])
    torch.save({'model_state_dict': model.state_dict()}, f'multi_model_new.pt')
    print('训练完毕！')
    return None




def getAccOnadataset(model:Multi_Model, data_loader): #测试模型的准确率
    model.eval()
    correct = 0
    with torch.no_grad():
        for i, data in enumerate(data_loader):
            input_ids, token_type_ids, attention_mask, img, label = data
            label = label.to(device)
            pre_y = multi_model(data)
            pre_y = torch.argmax(pre_y, dim=1)
            t = (pre_y == label).long().sum()
            correct += t
        correct = 1. * correct / len(data_loader.dataset)
    model.train()
    return correct.item()



if __name__ == '__main__':
    batch_size = 8
    # 使用Linux风格的路径
    path = './data/multimodal-cla' 
    samples_train = get_txt_img_lb(path, 'ID-label-train.txt')  # 读取训练集,3609
    samples_test = get_txt_img_lb(path, 'ID-label-test.txt')  # 读取测试集,902

    # 实例化训练集和测试集
    train_dataset = MyDataSet(samples_train)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_dataset = MyDataSet(samples_test)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

    torch.save(train_loader, 'train_loader')
    torch.save(test_loader, 'test_loader')

    '''
    '''
    # 3609+902 = 4511
    train_loader = torch.load('train_loader')
    test_loader = torch.load('test_loader')

    print(len(train_loader.dataset))
    print(len(test_loader.dataset))

    multi_model = Multi_Model().to(device)
    train(multi_model, train_loader)  # 对模型进行训练

    print('测试中..........')
    check_point = torch.load(f'multi_model_new.pt')  # 获取已训练的最好模型的参数
    multi_model.load_state_dict(check_point['model_state_dict'])
    acc_test = getAccOnadataset(multi_model, test_loader)
    print('在测试集上的准确率：{:.1f}%'.format(acc_test*100))



```

![image-20250906153933153](C:\Users\Gasoline\AppData\Roaming\Typora\typora-user-images\image-20250906153933153.png)
